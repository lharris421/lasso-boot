---
title: "Meeting Notes"
output:
  html_document:
    code_folding: hide
---

- For weekly meeting notes, try to just give a high level overview of things to discuss before, then update it with any other ideas after

# Running To-Do

- [ ] Look into faster versions of integration for univariate (grid, Guass Hermite, Huber loss)
- [ ] Implement method where bootstrap is also used to select lambda
- [ ] Figure out AWS bucket sharing
  - [x] Read: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html
    - [ ] Check if Patrick is admin, if not... may not work -> may just need to specify root instead?
  - [x] Read UIowa startup guide to get a better idea of what the account setup is structured as
    - Issue may be I am using version 2, not version 1 of command line tools
- [ ] Combine two sets of functions together (clean up)
- [ ] Read Gaussian Mirrors paper: https://doi.org/10.1080/01621459.2021.1923510
- [ ] Look at Univariate example when Beta is very small, say, beta = .01
- [ ] Profile code to see what is expensive in the Bayesian bootstrap
- [ ] Find a way to cleanly present the plots, maybe make ggplotly instead?
- [ ] Look into variational Bayes (again) for lasso, or generally if someone has tried to approximate the lasso posterior (numerically or analytically)
- [x] Create mre for "Error: Supplied lambda value(s) are outside the range of the model fit."
  - In process of creating realized the mistakes I was making
- [x] Move denominator into log to see if that would help
  - Didn't have an noticeable improvement
- [ ] Consider HPD method of interval finding -> likely too computationally demanding

# Week of Monday, September 11th, 2023

- Running into issues when applied to real data:
  - NAs occur when the bootstrap sample ends up with columns that are nonsingular (I just skip over them for now)
- MRE findings
  - I just had to heavily guide the lambda sequence
- CV first and once (seems promising)


# General notes

**U - shape coverage**: The reasoning for this seems to be that with a true value of zero, coverage is high as the penalization draws the estimates towards zero. For coefficients with small true values, they are susceptible to also being shrunk entirely to zero and they are effected by shrinkage in general. Although coefficients that are truly large also suffer from shrinkage, they are less likely to be drawn entirely to zero which is why we seem somewhat of a stabilization effect in the coverage.

**Bridge sampling**: Seems more like a method to compute the marginal likelihood which doesn't seem directly relevant for the current context since we really need to get directly at the posterior.

**Integrating from posterior mode**: Integrating from the posterior mode (the lasso solution) turned out to be not great when skewed


