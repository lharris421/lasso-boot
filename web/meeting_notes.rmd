---
title: "Meeting Notes"
output:
  html_document:
    code_folding: hide
---

- For weekly meeting notes, try to just give a high level overview of things to discuss before, then update it with any other ideas after

# Running To-Do

- [ ] Implement method where bootstrap is also used to select lambda
- [ ] Figure out AWS bucket sharing
  - [x] Read: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html
    - [x] Check if Patrick is admin, if not... may not work -> may just need to specify root instead?
  - [x] Read UIowa startup guide to get a better idea of what the account setup is structured as
    - Issue may be I am using version 2, not version 1 of command line tools
  - [ ] Reach out to Patrick's contact in IT?
  - [ ] json bucket policy generating function
- [ ] Read Gaussian Mirrors paper: https://doi.org/10.1080/01621459.2021.1923510
- [ ] Look at Univariate example when Beta is very small, say, beta = .01
- [ ] Consider HPD method of interval finding
- [ ] Speed up GenDataABN
- [ ] Add a global .gitignore
- [ ] Make better method names
- [x] Look into faster versions of integration for univariate (grid, Guass Hermite, Huber loss)
 - Grid didn't seem viable, but I have a fast solution now anyway
- [x] Combine two sets of functions together (clean up)
- [x] Profile code to see what is expensive in the Bayesian bootstrap
- [x] Create mre for "Error: Supplied lambda value(s) are outside the range of the model fit."
  - In process of creating realized the mistakes I was making
  
# Literature Search

- Confidence Intervals and Regions for the Lasso by Using Stochastic Variational Inequality Techniques in Optimization
  - https://doi.org/10.1111/rssb.12184
- Constructing confidence sets after lasso selection by randomized estimator augmentation
  - https://doi.org/10.48550/arXiv.1904.08018
  - Not sure about "after"
- Confidence Sets Based on Penalized Maximum Likelihood Estimators in Gaussian Regression
  - https://doi.org/10.48550/arXiv.0806.1652
  - Seems to focus more on standard intervals rather than develop new methods, but could be an interesting read
- Exact post-selection inference, with application to the lasso
  - https://doi.org/10.1214/15-AOS1371
  - Looks similar to selective inference
  - Ahh, well probably because it is
- High-Dimensional Inference: Confidence Intervals, p-Values and R-Software hdi
  - https://doi.org/10.1214/15-STS527
  - First cited paper for HDI package, seems very informative
  
  
- Interesting how much it seems there is a focus on debiased intervals...
  - With this, I feel like putting an emphasis on how often the intervals also contain the posterior mode would be beneficial
- Many of these also to seem to be rather complex and do not have packages.

- May need to make an argument about the width of the intervals alongside our argument that bias is okay
  - Consider interval width assuming known value of $\sigma^2$

# Week of Monday, September 25th

- I continue to have difficulty with AWS Login with V2
- I did figure out issue was having with "univariate" version, but now is superceeded
- Found that the marginal posterior is just a mixture of truncated normals

# Week of Monday, September 11th, 2023

- Running into issues when applied to real data:
  - NAs occur when the bootstrap sample ends up with columns that are nonsingular (I just skip over them for now)
- MRE findings
  - I just had to heavily guide the lambda sequence
- CV first and once (seems promising)

# General notes

**U - shape coverage**: The reasoning for this seems to be that with a true value of zero, coverage is high as the penalization draws the estimates towards zero. For coefficients with small true values, they are susceptible to also being shrunk entirely to zero and they are effected by shrinkage in general. Although coefficients that are truly large also suffer from shrinkage, they are less likely to be drawn entirely to zero which is why we seem somewhat of a stabilization effect in the coverage.

**Bridge sampling**: Seems more like a method to compute the marginal likelihood which doesn't seem directly relevant for the current context since we really need to get directly at the posterior.

**Integrating from posterior mode**: Integrating from the posterior mode (the lasso solution) turned out to be not great when skewed


