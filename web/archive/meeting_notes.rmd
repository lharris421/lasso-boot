---
title: "Notes"
output:
  html_document:
    code_folding: hide
---

- For weekly meeting notes, try to just give a high level overview of things to discuss before, then update it with any other ideas after

# Running To-Do

- [] Do computation on log scale
- [ ] Implement method where bootstrap is also used to select lambda
- [ ] Figure out AWS bucket sharing
  - [x] Read: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example2.html
    - [x] Check if Patrick is admin, if not... may not work -> may just need to specify root instead?
  - [x] Read UIowa startup guide to get a better idea of what the account setup is structured as
    - Issue may be I am using version 2, not version 1 of command line tools
  - [ ] Reach out to Patrick's contact in IT?
  - [ ] json bucket policy generating function
- [ ] Read Gaussian Mirrors paper: https://doi.org/10.1080/01621459.2021.1923510
- [ ] Look at Univariate example when Beta is very small, say, beta = .01
- [x] Consider HPD method of interval finding
- [ ] Speed up GenDataABN based on Patrick's updated to genData
- [ ] Add a global .gitignore
- [ ] Profile code in ncvreg to see what is expensive in the Bayesian bootstrap
  
# Literature Search

- Confidence Intervals and Regions for the Lasso by Using Stochastic Variational Inequality Techniques in Optimization
  - https://doi.org/10.1111/rssb.12184
- Constructing confidence sets after lasso selection by randomized estimator augmentation
  - https://doi.org/10.48550/arXiv.1904.08018
  - Not sure about "after"
- Confidence Sets Based on Penalized Maximum Likelihood Estimators in Gaussian Regression
  - https://doi.org/10.48550/arXiv.0806.1652
  - Seems to focus more on standard intervals rather than develop new methods, but could be an interesting read
- Exact post-selection inference, with application to the lasso
  - https://doi.org/10.1214/15-AOS1371
  - Looks similar to selective inference
  - Ahh, well probably because it is
- High-Dimensional Inference: Confidence Intervals, p-Values and R-Software hdi
  - https://doi.org/10.1214/15-STS527
  - First cited paper for HDI package, seems very informative
  
  
- Interesting how much it seems there is a focus on debiased intervals...
  - With this, I feel like putting an emphasis on how often the intervals also contain the posterior mode would be beneficial
- Many of these also to seem to be rather complex and do not have packages.

- May need to make an argument about the width of the intervals alongside our argument that bias is okay
  - Consider interval width assuming known value of $\sigma^2$

# General comments

**U - shape coverage**: The reasoning for this seems to be that with a true value of zero, coverage is high as the penalization draws the estimates towards zero. For coefficients with small true values, they are susceptible to also being shrunk entirely to zero and they are effected by shrinkage in general. Although coefficients that are truly large also suffer from shrinkage, they are less likely to be drawn entirely to zero which is why we seem somewhat of a stabilization effect in the coverage.

**Bridge sampling**: Seems more like a method to compute the marginal likelihood which doesn't seem directly relevant for the current context since we really need to get directly at the posterior.

**Integrating from posterior mode**: Integrating from the posterior mode (the lasso solution) turned out to be not great when skewed


