---
title: "Bootstrapping: ncvreg"
---

```{r setup, child = 'web/_include/setup.rmd'}
```


```{r, echo = FALSE}
progress = FALSE
met <- function(lam, sig2 = 1, n, truth) {
  # n <- length(truth)
  rate <- (n*lam) / sig2
  prior <- discrete_mixture(rate = rate, pzero = mean(truth == 0), n = length(truth))
  return(proxy_dist(prior, truth))
}
discrete_mixture <- function(pzero = .8, rate = 1, n = 1e5) {
  
  zeros <- rep(0, round(n*pzero))
  nonzero <- n - length(zeros)
  exp_quantiles <- qexp(1:nonzero / (nonzero + 1), rate = rate)
  return(c(zeros, exp_quantiles))
    
}
proxy_dist <- function(dist1, dist2) mean(abs(sort(abs(dist1)) - sort(abs(dist2))))
source("R/genDataABN.R")
```


# Set up test data

```{r}
hiv <- readRDS(url('https://s3.amazonaws.com/pbreheny-data-sets/Rhee2006.rds'))

y <- hiv$y
X <- hiv$X ## 361

## Get variables that are duplicated or constant
dup <- duplicated(t(X))
const <- apply(X, 2, function(x) length(unique(x)) == 1)

X <- X[,!dup & !const] ## 331
```

## Run cross validation

```{r}
hiv_cv <- cv.ncvreg(X, y, penalty = "lasso", seed = 042023)
hiv_fit <- hiv_cv$fit
```

## Consider residuals

```{r}
resid <- hiv_fit$y - predict(hiv_fit, lambda = hiv_cv$lambda.min, X = X)
plot(y, resid, xlab = "Y", ylab = "residual")
```

# Three Different Methods

## Pairs

```{r}
tmp <- boot_ncvreg(hiv_fit, lambda = hiv_cv$lambda.min, R = 100, seed = 1234, progress = progress)
tmp_boot_ci <- boot_ci_ncvreg(tmp, type = "percentile")
plot(tmp_boot_ci, n = 20)
```

## Residual

```{r}
tmp <- boot_ncvreg(hiv_fit, lambda = hiv_cv$lambda.min, R = 100, type = "residual", seed = 12345, progress = progress)
tmp_boot_ci <- boot_ci_ncvreg(tmp, type = "percentile")
plot(tmp_boot_ci, n = 20)
```

## Parametric

```{r}
tmp <- boot_ncvreg(hiv_cv, lambda = hiv_cv$lambda.min, R = 100, type = "parametric", seed = 1234, progress = progress)
tmp_boot_ci <- boot_ci_ncvreg(tmp, type = "percentile")
plot(tmp_boot_ci, n = 20)
```


# Simulations

## Setup

```{r}
library(hdrm)
set.seed(1234)
tmp_dat <- genDataABN(n=100, p=60, a=6, b=2, rho=0.5, beta=c(1,-1,0.5,-0.5,0.5,-0.5))
cv_fit <- cv.ncvreg(tmp_dat$X, tmp_dat$y, penalty = "lasso")
```

## Consider residuals

```{r}
tmp_fit <- cv_fit$fit
resid <- tmp_fit$y - predict(tmp_fit, lambda = cv_fit$lambda.min, X = tmp_dat$X)
plot(tmp_fit$y, resid, xlab ="Y", ylab = "residual")
```

## Examples

### Bootstrap: Pairs

```{r}
boot_pairs <- boot_ncvreg(cv_fit, lambda = cv_fit$lambda.min, R = 100, progress = progress)
boot_ci_pairs <- boot_ci_ncvreg(boot_pairs, type = "percentile", coverage = .8)
plot(boot_ci_pairs, n = 10)
```

### Bootstrap: Residual

```{r}
boot <- boot_ncvreg(cv_fit, lambda = cv_fit$lambda.min, R = 100, type = "residual", progress = progress)
boot_ci <- boot_ci_ncvreg(boot, type = "percentile", coverage = .8)
plot(boot_ci, n = 10)
```

### Bootstrap: Parametric

```{r}
boot_para <- boot_ncvreg(cv_fit, lambda = cv_fit$lambda.min, R = 100, type = "parametric", progress = progress)
boot_ci_para <- boot_ci_ncvreg(boot_para, type = "percentile", coverage = .8)
plot(boot_ci_para, n = 10)
```

## Run simulation

### Define functions

```{r}
sim <- function(type1, type2, lambda, recenter = FALSE, robust = FALSE,
                n = 100, p = 60, a = 6, b = 2, rho = 0.5, iterations = 100,
                R = 100, betas = c(1,-1,0.5,-0.5,0.5,-0.5), coverage = .8,
                correct = TRUE) {
  
  pb <- txtProgressBar(1, iterations, style=3)
  
  res <- matrix(nrow = iterations, ncol = p)
  for (i in 1:iterations) {
     
    tmp_dat <- genDataABN(n=n, p=p, a=a, b=b, rho=rho, beta=betas)
  
    if (lambda == "cv") {
      fit <- cv.ncvreg(tmp_dat$X, tmp_dat$y, penalty = "lasso")
      lambda <- fit$lambda.min; if (i == 1) print(lambda)
    } else {
      fit <- ncvreg(tmp_dat$X, tmp_dat$y, penalty = "lasso")  
    }
    
    if (lambda < min(fit$lambda) & correct) {lambda <- min(fit$lambda); print(lambda)}
    
    boot <- boot_ncvreg(fit, lambda = lambda, R = R, type = type1, progress = FALSE)
    boot_ci <- boot_ci_ncvreg(boot, type = type2, recenter = recenter, robust = robust, coverage = coverage)
  
    res[i,] <- tmp_dat$beta >= boot_ci$lower & tmp_dat$beta <= boot_ci$upper
    setTxtProgressBar(pb, i)
  }
  
  close(pb)
  return(res)
  
}

summarise_cov <- function(res, coverage, all_betas) {
  
  print(paste0("The overall coverage rate is: ", mean(res)))
  coverage_rates <- data.frame("coverage" = apply(res, 2, mean),
                               "beta" = abs(all_betas),
                               "Sign" = factor(sign(all_betas)))
  
  print({
    ggplot() +
      geom_point(coverage_rates, mapping = aes(x = beta, y = coverage, color = Sign)) +
      geom_hline(yintercept = coverage, color = "red") +
      theme_bw() +
      xlab(expression(abs(beta))) +
      ylab("Coverage Rate")
  })
  
}

eval_scenario <- function(name, type1, type2, lambda, coverage = .8, force = FALSE,
                          recenter = FALSE, robust = FALSE,
                          n = 100, p = 60, a = 6, b = 2, rho = 0.5, 
                          R = 100, iterations = 100, betas = c(1,-1,0.5,-0.5,0.5,-0.5), seed = 1234) {
  
  tmp_dat <- genDataABN(n=n, p=p, a=a, b=b, rho=rho, beta=betas)
  all_betas <- tmp_dat$beta
  nm <- paste0(c(name, type1, type2, coverage, lambda, n, p, a, b, rho, R, iterations, ".rds"), collapse = "_")
  
  all_files <- list.files("/Users/loganharris/github/dissertation/storage")
  
  if (!(nm %in% all_files) | force) {
    
    set.seed(seed)
    tmp <- sim(type1, type2, recenter = recenter, robust = robust,
                lambda = lambda,
                n = n, p = p, a = a, b = b, rho = rho, 
                R = R, betas = betas, coverage = coverage)
    save(tmp, file = paste0("/Users/loganharris/github/dissertation/storage/", nm))
    summarise_cov(tmp, coverage = coverage, all_betas)
    
  } else {
    
    load(paste0("/Users/loganharris/github/dissertation/storage/", nm))
    summarise_cov(tmp, coverage = coverage, all_betas)
    
  }
  
}
```


### Initial

Previous work I have done showed that the method for computing intervals seemed less important that the method used for the bootstrap. I may revisit this later, but for now I am focusing on the different methods for bootstrapping. Even from here, for now I will focus on the "pairs" bootstrap.

```{r}
p <- 60
n <- 100
b <- 2
betas <- c(1,-1,0.5,-0.5,0.5,-0.5)
a <- length(betas)

truth <- c(betas, rep(0, p - length(betas)))
(lambda <- optimize(f = met, c(1e-4, 1e4), truth = truth, n = n)$minimum)
```

```{r}
eval_scenario("initial", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, seed = 43211234, lambda = lambda)
```

### Larger N

What happens if under the same conditions we increase the sample size?

```{r}
n <- 500
(lambda <- optimize(f = met, c(1e-4, 1e4), truth = truth, n = n)$minimum)
eval_scenario("largerN", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, iterations = 100, lambda = lambda, coverage = .8)
```

This performs much better.

### Increasing values of betas

```{r}
p <- 100
n <- 160
b <- 2
betas <- c(seq(0.5, 5, by = .5)) * rep(c(1, -1), 5)
a <- length(betas)

truth <- c(betas, rep(0, p - length(betas)))
(lambda <- optimize(f = met, c(1e-3, 1e3), truth = truth, n = n)$minimum)
```


```{r, eval = FALSE}
eval_scenario("increasing", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = .007, force = TRUE)
```
## Alternative ways of calculating lambda


### Estimation Consistency

```{r}
prob <- .99
p <- 100
n <- 160
c <- log((prob - 1) / -2) * (-2 / log(p)) + 2
(lambda <- 2*sqrt(c*log(p)/n))
```

```{r}
eval_scenario("ec", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = lambda)
```
```{r}
rt <- function(lam, n, sig2 = 1) {
  (n*lam) / sig2
}

xaxs <- seq(0, 10, by = .1)
plot(x = xaxs, y = dexp(xaxs, rate = rt(lambda, n)), type = "l")
```

### Via CV

```{r}
eval_scenario("cv", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = "cv", force = TRUE)
```

## Evaluating Distance

```{r}
n <- 300
a <- p <- 200
b <- 0
betas <- qexp(1:p / (p+1), rate = rt(.015, n = 300))

hist(betas)

truth <- c(betas, rep(0, p - length(betas)))
lambda <- optimize(f = met, c(1e-3, 1e3), truth = truth, n = n)$minimum ## 0.015

eval_scenario("dist", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = lambda)
eval_scenario("dist", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = "cv")
```

## Uniform comparison

```{r}
p <- 1000
prior <- qexp(1:p / (p+1), rate = rt(.015, n = n))
unif <- qunif(1:p / (p+1), min = -1, max = 1)
proxy_dist(prior, unif)

p <- 200
betas <- qunif(1:p / (p+1), min = -1, max = 1)

truth <- c(betas, rep(0, p - length(betas)))
lambda <- optimize(f = met, c(1e-3, 1e3), truth = truth, n = n)$minimum ## 0.0068

eval_scenario("dist", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = lambda)
eval_scenario("uniform", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, lambda = "cv")
```

Need to come back and double check I didn't switch n and p anywhere else