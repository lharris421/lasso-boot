---
title: "Bootstrapping: ncvreg"
output:
  html_document:
    code_folding: hide
---

```{r setup, child = 'web/_include/setup.rmd'}
```


```{r, echo = FALSE}
progress = FALSE

## change to load all
devtools::load_all()
```


# Set up test data

```{r}
hiv <- readRDS(url('https://s3.amazonaws.com/pbreheny-data-sets/Rhee2006.rds'))

y <- hiv$y
X <- hiv$X ## 361

## Get variables that are duplicated or constant
dup <- duplicated(t(X))
const <- apply(X, 2, function(x) length(unique(x)) == 1)

X <- X[,!dup & !const] ## 331
```

## Run cross validation

```{r}
hiv_cv <- cv.ncvreg(X, y, penalty = "lasso", seed = 042023)
hiv_fit <- hiv_cv$fit
```

## Consider residuals

```{r}
resid <- hiv_fit$y - predict(hiv_fit, lambda = hiv_cv$lambda.min, X = X)
plot(y, resid, xlab = "Y", ylab = "residual")
```

# Four Different Methods

For this example data set, the three methods still produce noticeably different results. In general, the reasoning behind using "pairs" in a high dimensional setting seems to be most logical and will be the main focus in the near future. This is two fold: 1) Sampling cases implies that $\boldsymbol{X}$ is random which is likely always the case in high dimensional settings and 2) it makes the fewest assumptions. The other two both at a minimum make an assumption of homoscedasticity which is assuredly the case in a high dimensional setting.

## Pairs

```{r}
tmp <- boot_ncvreg(hiv_fit, lambda = hiv_cv$lambda.min, R = 100, seed = 1234, progress = progress)
tmp_boot_ci <- boot_ci_ncvreg(tmp, type = "percentile")
plot(tmp_boot_ci, n = 20)
```

## Residual

```{r}
tmp <- boot_ncvreg(hiv_fit, lambda = hiv_cv$lambda.min, R = 100, type = "residual", seed = 12345, progress = progress)
tmp_boot_ci <- boot_ci_ncvreg(tmp, type = "percentile")
plot(tmp_boot_ci, n = 20)
```

## Parametric

```{r}
tmp <- boot_ncvreg(hiv_cv, lambda = hiv_cv$lambda.min, R = 100, type = "parametric", seed = 1234, progress = progress)
tmp_boot_ci <- boot_ci_ncvreg(tmp, type = "percentile")
plot(tmp_boot_ci, n = 20)
```

## Rx

```{r}
tmp <- boot_ncvreg(hiv_cv, lambda = hiv_cv$lambda.min, R = 100, type = "Rx", seed = 1234, progress = progress)
tmp_boot_ci <- boot_ci_ncvreg(tmp, type = "percentile")
plot(tmp_boot_ci, n = 20)
```


# Simulations

## Setup

```{r}
library(hdrm)
## set.seed(1234)
tmp_dat <- genDataABN(n=100, p=60, a=6, b=2, rho=0.5, beta=c(1,-1,0.5,-0.5,0.5,-0.5))
cv_fit <- cv.ncvreg(tmp_dat$X, tmp_dat$y, penalty = "lasso")
```

## Consider residuals

```{r}
tmp_fit <- cv_fit$fit
resid <- tmp_fit$y - predict(tmp_fit, lambda = cv_fit$lambda.min, X = tmp_dat$X)
y <- tmp_dat$y
plot(tmp_fit$y, resid, xlab ="Y", ylab = "residual")
```




## Examples

### Bootstrap: Pairs

```{r}
boot_pairs <- boot_ncvreg(cv_fit, lambda = cv_fit$lambda.min, R = 100, progress = progress)
boot_ci_pairs <- boot_ci_ncvreg(boot_pairs, type = "percentile", coverage = .8)
plot(boot_ci_pairs, n = 10)
```

### Bootstrap: Pairs + Select Lambda

```{r}
fit <- ncvreg(tmp_dat$X, tmp_dat$y, penalty = "lasso")
boot_pairs <- boot_ncvreg(fit, lambda = "boot", progress = progress); boot_pairs$lambda
boot_ci_pairs <- boot_ci_ncvreg(boot_pairs, type = "percentile", coverage = .8)
plot(boot_ci_pairs, n = 10)
```

### Bootstrap: Residual

```{r}
boot <- boot_ncvreg(cv_fit, lambda = cv_fit$lambda.min, R = 100, type = "residual", progress = progress)
boot_ci <- boot_ci_ncvreg(boot, type = "percentile", coverage = .8)
plot(boot_ci, n = 10)
```

### Bootstrap: Parametric

```{r}
boot_para <- boot_ncvreg(cv_fit, lambda = cv_fit$lambda.min, R = 100, type = "parametric", progress = progress)
boot_ci_para <- boot_ci_ncvreg(boot_para, type = "percentile", coverage = .8)
plot(boot_ci_para, n = 10)
```

### Rx

```{r}
boot_rx <- boot_ncvreg(cv_fit, lambda = cv_fit$lambda.min, R = 100, type = "Rx", progress = progress)
boot_ci_rx <- boot_ci_ncvreg(boot_rx, type = "percentile", coverage = .8)
plot(boot_ci_rx, n = 10)
```

## Run simulation

### Define functions

```{r}
sim <- function(type1, type2, lambda, recenter = FALSE, robust = FALSE,
                n = 100, p = 60, a = 6, b = 2, rho = 0.5, iterations = 100,
                R = 100, betas = c(1,-1,0.5,-0.5,0.5,-0.5), coverage = .8,
                correct = TRUE) {
  
  pb <- txtProgressBar(1, iterations, style=3)
  
  res <- matrix(nrow = iterations, ncol = p)
  for (i in 1:iterations) {
     
    tmp_dat <- genDataABN(n=n, p=p, a=a, b=b, rho=rho, beta=betas)
  
    if (lambda == "cv") {
      fit <- cv.ncvreg(tmp_dat$X, tmp_dat$y, penalty = "lasso")
      lambda <- fit$lambda.min; if (i == 1) print(lambda)
    } else {
      fit <- ncvreg(tmp_dat$X, tmp_dat$y, penalty = "lasso")  
    }
    
    if (lambda < min(fit$lambda) & correct) {lambda <- min(fit$lambda); print(lambda)}
    
    boot <- boot_ncvreg(fit, lambda = lambda, R = R, type = type1, progress = FALSE)
    boot_ci <- boot_ci_ncvreg(boot, type = type2, recenter = recenter, robust = robust, coverage = coverage)
  
    res[i,] <- tmp_dat$beta >= boot_ci$lower & tmp_dat$beta <= boot_ci$upper
    setTxtProgressBar(pb, i)
  }
  
  close(pb)
  return(res)
  
}

summarise_cov <- function(res, coverage, all_betas) {
  
  print(paste0("The overall coverage rate is: ", mean(res)))
  coverage_rates <- data.frame("coverage" = apply(res, 2, mean),
                               "beta" = abs(all_betas),
                               "Sign" = factor(sign(all_betas)))
  
  print({
    ggplot() +
      geom_point(coverage_rates, mapping = aes(x = beta, y = coverage, color = Sign)) +
      geom_hline(yintercept = coverage, color = "red") +
      theme_bw() +
      xlab(expression(abs(beta))) +
      ylab("Coverage Rate")
  })
  
}

eval_scenario <- function(name, type1, type2, lambda, coverage = .8, force = FALSE,
                          recenter = FALSE, robust = FALSE,
                          n = 100, p = 60, a = 6, b = 2, rho = 0.5, 
                          R = 100, iterations = 100, betas = c(1,-1,0.5,-0.5,0.5,-0.5), seed = 1234) {
  
  tmp_dat <- genDataABN(n=n, p=p, a=a, b=b, rho=rho, beta=betas)
  all_betas <- tmp_dat$beta
  nm <- paste0(c(name, type1, type2, coverage, lambda, n, p, a, b, rho, R, iterations, ".rds"), collapse = "_")
  
  all_files <- list.files("/Users/loganharris/github/lasso-boot/storage")
  
  if (!(nm %in% all_files) | force) {
    
    set.seed(seed)
    tmp <- sim(type1, type2, recenter = recenter, robust = robust,
                lambda = lambda,
                n = n, p = p, a = a, b = b, rho = rho, 
                R = R, betas = betas, coverage = coverage)
    save(tmp, file = paste0("/Users/loganharris/github/lasso-boot/storage/", nm))
    summarise_cov(tmp, coverage = coverage, all_betas)
    
  } else {
    
    load(paste0("/Users/loganharris/github/lasso-boot/storage/", nm))
    summarise_cov(tmp, coverage = coverage, all_betas)
    
  }
  
}
```


### Initial

Previous work I have done showed that the method for computing intervals seemed less important that the method used for the bootstrap. I may revisit this later, but for now I am focusing on the different methods for bootstrapping. Even from here, for now I will focus on the "pairs" bootstrap.

```{r}
p <- 60
n <- 100
b <- 2
betas <- c(1,-1,0.5,-0.5,0.5,-0.5)
a <- length(betas)
```


#### Boot + Pairs

```{r}
eval_scenario("boot_lambda", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, seed = 43211234, lambda = "boot")
```

#### CV + Rx

```{r}
eval_scenario("boot_lambda", type1 = "Rx", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, seed = 43211234, lambda = "cv")
```

### Larger N

What happens if under the same conditions we increase the sample size?

```{r}
n <- 500
eval_scenario("largerN", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, iterations = 100, lambda = "boot", coverage = .8)
```

### Increasing values of betas

```{r}
p <- 100
n <- 160
b <- 2
betas <- c(seq(0.5, 5, by = .5)) * rep(c(1, -1), 5)
a <- length(betas)
```



```{r, eval = FALSE}
eval_scenario("increasing", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = "boot")
```


# Evaluating Distance

## Exponential

Here is an example where the true betas are generated from an exponential prior.

### Optimize

```{r}
n <- 300
a <- p <- 200
b <- 0
betas <- qexp(1:p / (p+1), rate = rt(.015, n = 300))

truth <- c(rep(0, p - length(betas)), betas)
(lambda <- optimize(met, interval = c(1e-3, 1e2), n = n, truth = truth)$minimum)
```

```{r}
eval_scenario("dist", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = lambda)
```

### CV

```{r}
eval_scenario("dist", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = "cv")
```

### Boot

```{r}
eval_scenario("dist", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, lambda = "boot")
```


## Uniform

```{r}
n <- 300
a <- p <- 200
b <- 0
prior <- qexp(1:p / (p+1), rate = rt(.015, n = n))
unif <- qunif(1:p / (p+1), min = -1, max = 1)
print(paste0("Distance to prior: ", round(proxy_dist(prior, unif), 3)))

betas <- qunif(1:p / (p+1), min = -1, max = 1)
```

### Optimize

```{r}
truth <- c(rep(0, p - length(betas)), betas)
(lambda <- optimize(met, interval = c(1e-3, 1e2), n = n, truth = truth)$minimum)
eval_scenario("uniform", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, lambda = lambda)
```

### CV

```{r}
eval_scenario("uniform", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, lambda = "cv")
```

### Boot

```{r}
eval_scenario("uniform", type1 = "pairs", type2 = "percentile", n = n, p = p, a = a, b = b, betas = betas, R = 100, lambda = "boot")
```
